{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b988577-70bd-43c1-bb28-2507436b2a1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install vit_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48c77c9-c6af-49b4-9bb7-4a3274a3efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the image\n",
    "image_path = 'image_slice.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "image_tensor = preprocess(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c51fff-7170-4351-8f11-50b81b06324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.twins_svt import TwinsSVT\n",
    "\n",
    "model = TwinsSVT(\n",
    "    num_classes=1000,       # number of output classes\n",
    "    s1_emb_dim=64,          # stage 1 - patch embedding projected dimension\n",
    "    s2_emb_dim=128,         # stage 2 - patch embedding projected dimension\n",
    "    s3_emb_dim=256,         # stage 3 - patch embedding projected dimension\n",
    "    s4_emb_dim=512,         # stage 4 - patch embedding projected dimension\n",
    "    s1_depth=1,\n",
    "    s2_depth=1,\n",
    "    s3_depth=5,\n",
    "    s4_depth=4,\n",
    "    peg_kernel_size=3,      # positional encoding generator kernel size\n",
    "    dropout=0.              # dropout\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "image_tensor = image_tensor.to(device)\n",
    "\n",
    "# Forward pass to get image embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "    image_embeddings = outputs  # Shape: (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00b7dda0-a816-4aca-bd48-35e7001282cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "class ClassifierHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the classifier head with correct input dimension\n",
    "num_classes = 1000\n",
    "classifier_head = ClassifierHead(1000, num_classes).to(device)\n",
    "\n",
    "# Assuming the outputs from the TwinsSVT model need to be flattened\n",
    "image_embeddings_flattened = image_embeddings.view(image_embeddings.size(0), -1)\n",
    "\n",
    "# image_embeddings_flattened.shape\n",
    "print(image_embeddings_flattened.shape)\n",
    "\n",
    "# Forward pass through classifier head\n",
    "logits = classifier_head(image_embeddings_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fa1ad50-86b6-4e7e-b643-e979b1e03b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1175, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 1175 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(gene_embeddings_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Combine the embeddings\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m combined_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeddings_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_embeddings_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Concatenate along the feature dimension\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 1175 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the gene embeddings\n",
    "gene_embeddings_path = 'gene_embeddings.csv'\n",
    "gene_embeddings = pd.read_csv(gene_embeddings_path)\n",
    "\n",
    "# Convert gene embeddings to tensor and adjust shape if needed\n",
    "gene_embeddings_tensor = torch.tensor(gene_embeddings.values, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# Ensure gene embeddings and image embeddings have the same batch size\n",
    "batch_size = image_embeddings.size(0)\n",
    "gene_embeddings_tensor = gene_embeddings_tensor.repeat(batch_size, 1, 1)\n",
    "\n",
    "gene_embeddings_tensor = torch.reshape(gene_embeddings_tensor, (1175, 512))\n",
    "\n",
    "# gene_embeddings_tensor.shape\n",
    "print(gene_embeddings_tensor.shape)\n",
    "\n",
    "# Combine the embeddings\n",
    "combined_embeddings = torch.cat((image_embeddings_flattened, gene_embeddings_tensor), dim=-1)  # Concatenate along the feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0639335f-5ba8-40b7-9aa0-aea5b58f63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "optimizer = Adam(list(model.parameters()) + list(classifier_head.parameters()), lr=1e-5)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "# Dummy labels for demonstration purposes\n",
    "labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = classifier_head(combined_embeddings)\n",
    "    loss = criterion(logits, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f62e8c-0b14-4959-ad4a-4145c4eeddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "with torch.no_grad():\n",
    "    combined_output = combined_embeddings.view(batch_size, -1)\n",
    "    logits = classifier_head(combined_output)\n",
    "    probs = logits.softmax(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
